---

title: brown_phototour_revisited


keywords: fastai
sidebar: home_sidebar

summary: "The package for local patch descriptors evaluation, which takes into account image indexes  and second nearest neighbor ratio (SNN) filtering strategy. So, it evaluates descriptors in a similar way, how they are used in practice. It is in agreement with <a href='https://arxiv.org/abs/2003.01587'>IMC benchmark</a>, unlike the original protocol. The benchmark is not "test benchmark" by amy means. Rather it is intended to be used as validation/development set for local patch descriptor learning and/or crafting."
description: "The package for local patch descriptors evaluation, which takes into account image indexes  and second nearest neighbor ratio (SNN) filtering strategy. So, it evaluates descriptors in a similar way, how they are used in practice. It is in agreement with <a href='https://arxiv.org/abs/2003.01587'>IMC benchmark</a>, unlike the original protocol. The benchmark is not "test benchmark" by amy means. Rather it is intended to be used as validation/development set for local patch descriptor learning and/or crafting."
nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>pip install brown_phototour_revisited</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is a single function, which does everything for you: <a href="/brown_phototour_revisited/benchmarking.html#full_evaluation"><code>full_evaluation</code></a>. The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary.  We are following it here as well.</p>
<p>However, if you need to run some tests separately, or reuse some functions -- we will cover the usage below.
In the following example we will show how to use <a href="/brown_phototour_revisited/benchmarking.html#full_evaluation"><code>full_evaluation</code></a> to evaluate SIFT descriptor as implemented in kornia.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Requirement already satisfied: kornia in /home/mishkdmy/.local/lib/python3.7/site-packages (0.4.1+0c9e625)
Requirement already satisfied: numpy in /home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages (from kornia) (1.18.0)
Requirement already satisfied: torch&lt;1.7.0,&gt;=1.6.0 in /home/mishkdmy/.local/lib/python3.7/site-packages (from kornia) (1.6.0)
Requirement already satisfied: future in /home/mishkdmy/.local/lib/python3.7/site-packages (from torch&lt;1.7.0,&gt;=1.6.0-&gt;kornia) (0.18.2)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">kornia</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="kn">from</span> <span class="nn">brown_phototour_revisited.benchmarking</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">65</span> 

<span class="n">model</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">SIFTDescriptor</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">rootsift</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">descs_out_dir</span> <span class="o">=</span> <span class="s1">&#39;data/descriptors&#39;</span>
<span class="n">download_dataset_to</span> <span class="o">=</span> <span class="s1">&#39;data/dataset&#39;</span>
<span class="n">results_dir</span> <span class="o">=</span> <span class="s1">&#39;data/mAP&#39;</span>

<span class="n">results_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">results_dict</span><span class="p">[</span><span class="s1">&#39;Kornia RootSIFT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">full_evaluation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                <span class="s1">&#39;Kornia RootSIFT&#39;</span><span class="p">,</span>
                                <span class="n">path_to_save_dataset</span> <span class="o">=</span> <span class="n">download_dataset_to</span><span class="p">,</span>
                                <span class="n">path_to_save_descriptors</span> <span class="o">=</span> <span class="n">descs_out_dir</span><span class="p">,</span>
                                <span class="n">path_to_save_mAP</span> <span class="o">=</span> <span class="n">results_dir</span><span class="p">,</span>
                                <span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span><span class="p">,</span> 
                                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">),</span> 
                           <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span>
                           <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;pytorch-cuda&#39;</span><span class="p">)</span>
<span class="n">clear_output</span><span class="p">()</span>
<span class="n">print_results_table</span><span class="p">(</span><span class="n">results_dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia RootSIFT        56.70              47.71               48.09 
------------------------------------------------------------------------------
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Precomputed-benchmark-results">Precomputed benchmark results<a class="anchor-link" href="#Precomputed-benchmark-results"> </a></h2><p>We have pre-computed some results for you. The implementation is in the following notebooks in the <a href="examples/">examples</a> dir:</p>
<ul>
<li><a href="examples/evaluate_deep_descriptors.ipynb">Deep descriptors</a></li>
<li><a href="examples/evaluate_non_deep_descriptors.ipynb">Non-deep descriptors</a></li>
</ul>
<p>The final tables are the following:</p>

<pre><code>------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia RootSIFT 32px   58.24              49.07               49.65 
HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64
SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65
TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24
SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63
HardNetPS 32px         55.56              49.70               49.12 
R2D2_center_grayscal   61.47              53.18               54.98 
R2D2_MeanCenter_gray   62.73              54.10               56.17 
------------------------------------------------------------------------------

------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia RootSIFT 32px   58.24              49.07               49.65 
Kornia RootSIFT 41px   57.83              48.48               49.01 
Kornia SIFT 32px       58.47              47.76               48.70 
Kornia SIFT 41px       58.14              47.30               48.30 
OpenCV_SIFT 32px       53.16              45.93               46.00 
OpenCV_SIFT 41px       54.10              46.09               46.29 
OpenCV_RootSIFT 32px   53.50              47.16               47.37 
OpenCV_RootSIFT 41px   54.19              47.20               47.37 
OpenCV_LATCH 65px  -----  -----        -----  37.26        -----  39.08
OpenCV_LUCID 32px      20.37              23.08               27.24 
skimage_BRIEF 65px     52.68              44.82               46.56 
------------------------------------------------------------------------------



</code></pre>
<h3 id="Disclaimer-1:-don't-trust-this-table-fully">Disclaimer 1: don't trust this table fully<a class="anchor-link" href="#Disclaimer-1:-don't-trust-this-table-fully"> </a></h3><p>I haven't (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true -- and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please open an issue. Thank you.</p>
<h3 id="Disclaimer-2:-it-is-not-&quot;benchmark&quot;.">Disclaimer 2: it is not "benchmark".<a class="anchor-link" href="#Disclaimer-2:-it-is-not-&quot;benchmark&quot;."> </a></h3><p>The intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results  instead of doing so on <a href="https://github.com/hpatches/hpatches-benchmark">HPatches</a>. After you have finished tuning, please, evaluate your local descriptors on some downstream task like <a href="https://github.com/vcg-uvic/image-matching-benchmark">IMC image matching benchmark</a> or <a href="https://www.visuallocalization.net/">visual localization</a>.</p>
<p><strong>If you found any mistake, please open an issue</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Detailed-examples-of-usage">Detailed examples of usage<a class="anchor-link" href="#Detailed-examples-of-usage"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are 3 main modules of the package: <a href="/brown_phototour_revisited/dataset.html"><code>dataset</code></a>, <a href="/brown_phototour_revisited/extraction.html"><code>extraction</code></a> and <a href="/brown_phototour_revisited/benchmarking.html"><code>benchmarking</code></a>.</p>
<p>To run the benchmark manually one needs two things:</p>
<ul>
<li>extract the descriptors with either <a href="/brown_phototour_revisited/extraction.html#extract_pytorchinput_descriptors"><code>extract_pytorchinput_descriptors</code></a> or <a href="/brown_phototour_revisited/extraction.html#extract_numpyinput_descriptors"><code>extract_numpyinput_descriptors</code></a></li>
<li>get the mean average precision (mAP) with <a href="/brown_phototour_revisited/benchmarking.html#evaluate_mAP_snn_based"><code>evaluate_mAP_snn_based</code></a></li>
</ul>
<p>Here we will show how to evaluate several descriptors: Pytorch-based HardNet, OpenCV SIFT, skimage BRIEF.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code below will download the HardNet, trained on Liberty dataset, download the Notredame subset and extracts the local patch descriptors into the dict. Note, that we should not evaluate descriptor on the same subset, as it was trained on.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">kornia</span>

<span class="kn">from</span> <span class="nn">brown_phototour_revisited.dataset</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">brown_phototour_revisited.extraction</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">brown_phototour_revisited.benchmarking</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">HardNet</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">descs_out_dir</span> <span class="o">=</span> <span class="s1">&#39;data/descriptors&#39;</span>
<span class="n">download_dataset_to</span> <span class="o">=</span> <span class="s1">&#39;data/dataset&#39;</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">32</span> <span class="c1"># HardNet expects 32x32 patches</span>

<span class="n">desc_dict</span> <span class="o">=</span> <span class="n">extract_pytorchinput_descriptors</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                <span class="s1">&#39;HardNet+Liberty&#39;</span><span class="p">,</span>
                                <span class="n">subset</span><span class="o">=</span> <span class="s1">&#39;notredame&#39;</span><span class="p">,</span> 
                                <span class="n">path_to_save_dataset</span> <span class="o">=</span> <span class="n">download_dataset_to</span><span class="p">,</span>
                                <span class="n">path_to_save_descriptors</span> <span class="o">=</span> <span class="n">descs_out_dir</span><span class="p">,</span>
                                <span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span><span class="p">,</span> 
                                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre># Found cached data data/dataset/notredame.pt
data/descriptors/HardNet+Liberty_32px_notredame.npy already exists, loading
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="n">desc_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>dict_keys([&#39;descriptors&#39;, &#39;labels&#39;, &#39;img_idxs&#39;])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Function <a href="/brown_phototour_revisited/extraction.html#extract_pytorchinput_descriptors"><code>extract_pytorchinput_descriptors</code></a> expects <code>torch.nn.Module</code>, which takes <code>(B, 1, patch_size, patch_size)</code> <code>torch.Tensor</code> input and outputs <code>(B, desc_dim)</code> <code>torch.Tensor</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we will calculate mAP.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mAP</span> <span class="o">=</span> <span class="n">evaluate_mAP_snn_based</span><span class="p">(</span><span class="n">desc_dict</span><span class="p">[</span><span class="s1">&#39;descriptors&#39;</span><span class="p">],</span>
                             <span class="n">desc_dict</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> 
                             <span class="n">desc_dict</span><span class="p">[</span><span class="s1">&#39;img_idxs&#39;</span><span class="p">],</span>
                             <span class="n">path_to_save_mAP</span> <span class="o">=</span> <span class="s1">&#39;data/mAP/HardNet+Liberty_notredame.npy&#39;</span><span class="p">,</span>
                            <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;pytorch-cuda&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;HardNetLib mAP on Notredame = </span><span class="si">{</span><span class="n">mAP</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Found saved results data/mAP/HardNet+Liberty_notredame.npy, loading
HardNetLib mAP on Notredame = 0.61901
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we will evaluate OpenCV SIFT descriptor. 
Function <a href="/brown_phototour_revisited/extraction.html#extract_numpyinput_descriptors"><code>extract_numpyinput_descriptors</code></a> expects function or object, which takes (patch_size, patch_size) input and outputs (desc_dim) np.array.</p>
<p>As OpenCV doesn't provide such function, we will create it ourselves, or rather take already implemented from <a href="https://github.com/hpatches/hpatches-benchmark/blob/master/python/extract_opencv_sift.py#L43">HPatches benchmark repo</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">65</span>

<span class="c1"># https://github.com/hpatches/hpatches-benchmark/blob/master/python/extract_opencv_sift.py#L43</span>
<span class="k">def</span> <span class="nf">get_center_kp</span><span class="p">(</span><span class="n">PS</span><span class="o">=</span><span class="mf">65.</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">PS</span><span class="o">/</span><span class="mf">2.0</span>
    <span class="n">center_kp</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">KeyPoint</span><span class="p">()</span>
    <span class="n">center_kp</span><span class="o">.</span><span class="n">pt</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
    <span class="n">center_kp</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">PS</span><span class="o">/</span><span class="mf">5.303</span>
    <span class="k">return</span> <span class="n">center_kp</span>


<span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT_create</span><span class="p">()</span>
<span class="n">center_kp</span> <span class="o">=</span> <span class="n">get_center_kp</span><span class="p">(</span><span class="n">patch_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">extract_opencv_sift</span><span class="p">(</span><span class="n">patch</span><span class="p">):</span>
    <span class="c1">#Convert back to UINT8 and provide aux keypoint in the center of the patch</span>
    <span class="k">return</span> <span class="n">sift</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">255</span><span class="o">*</span><span class="n">patch</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">),[</span><span class="n">center_kp</span><span class="p">])[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="n">descs_out_dir</span> <span class="o">=</span> <span class="s1">&#39;data/descriptors&#39;</span>
<span class="n">download_dataset_to</span> <span class="o">=</span> <span class="s1">&#39;data/dataset&#39;</span>


<span class="n">desc_dict_sift</span> <span class="o">=</span> <span class="n">extract_numpyinput_descriptors</span><span class="p">(</span><span class="n">extract_opencv_sift</span><span class="p">,</span>
                                <span class="s1">&#39;OpenCV_SIFT&#39;</span><span class="p">,</span>
                                <span class="n">subset</span><span class="o">=</span> <span class="s1">&#39;notredame&#39;</span><span class="p">,</span> 
                                <span class="n">path_to_save_dataset</span> <span class="o">=</span> <span class="n">download_dataset_to</span><span class="p">,</span>
                                <span class="n">path_to_save_descriptors</span> <span class="o">=</span> <span class="n">descs_out_dir</span><span class="p">,</span>
                                <span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre># Found cached data data/dataset/notredame.pt
data/descriptors/OpenCV_SIFT_65px_notredame.npy already exists, loading
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mAP_SIFT</span> <span class="o">=</span> <span class="n">evaluate_mAP_snn_based</span><span class="p">(</span><span class="n">desc_dict_sift</span><span class="p">[</span><span class="s1">&#39;descriptors&#39;</span><span class="p">],</span>
                             <span class="n">desc_dict_sift</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> 
                             <span class="n">desc_dict_sift</span><span class="p">[</span><span class="s1">&#39;img_idxs&#39;</span><span class="p">],</span>
                            <span class="n">path_to_save_mAP</span> <span class="o">=</span> <span class="s1">&#39;data/mAP/OpenCV_SIFT65_notredame.npy&#39;</span><span class="p">,</span>
                            <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;pytorch-cuda&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;OpenCV SIFT PS = </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s1">, mAP on Notredame = </span><span class="si">{</span><span class="n">mAP_SIFT</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Found saved results data/mAP/OpenCV_SIFT65_notredame.npy, loading
OpenCV SIFT PS = 65, mAP on Notredame = 0.45530
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, let's try some binary descriptor, like BRIEF. Evaluation so far supports two metrics: <code>"euclidean"</code> and
<code>"hamming"</code>.</p>
<p>Function <a href="/brown_phototour_revisited/extraction.html#extract_numpyinput_descriptors"><code>extract_numpyinput_descriptors</code></a> expects function or object, which takes <code>(patch_size, patch_size)</code> input and outputs <code>(desc_dim)</code> <code>np.array</code>.</p>
<p>As skimage doesn't provide exactly such function, we will create it ourselves by placing "detected" keypoint in the center of the patch.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">BRIEF</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">65</span>
<span class="n">BR</span> <span class="o">=</span> <span class="n">BRIEF</span><span class="p">(</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">extract_skimage_BRIEF</span><span class="p">(</span><span class="n">patch</span><span class="p">):</span>
    <span class="n">BR</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="n">patch</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">patch_size</span><span class="o">/</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">/</span><span class="mf">2.0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">BR</span><span class="o">.</span><span class="n">descriptors</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">desc_dict_brief</span> <span class="o">=</span> <span class="n">extract_numpyinput_descriptors</span><span class="p">(</span><span class="n">extract_skimage_BRIEF</span><span class="p">,</span>
                                <span class="s1">&#39;skimage_BRIEF&#39;</span><span class="p">,</span>
                                <span class="n">subset</span><span class="o">=</span> <span class="s1">&#39;notredame&#39;</span><span class="p">,</span> 
                                <span class="n">path_to_save_dataset</span> <span class="o">=</span> <span class="n">download_dataset_to</span><span class="p">,</span>
                                <span class="n">path_to_save_descriptors</span> <span class="o">=</span> <span class="n">descs_out_dir</span><span class="p">,</span>
                                <span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre># Found cached data data/dataset/notredame.pt
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='468159' class='' max='468159' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [468159/468159 07:26<00:00]
    </div>
    
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's will take a while.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mAP_BRIEF</span> <span class="o">=</span> <span class="n">evaluate_mAP_snn_based</span><span class="p">(</span><span class="n">desc_dict_brief</span><span class="p">[</span><span class="s1">&#39;descriptors&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
                             <span class="n">desc_dict_brief</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> 
                             <span class="n">desc_dict_brief</span><span class="p">[</span><span class="s1">&#39;img_idxs&#39;</span><span class="p">],</span>
                             <span class="n">path_to_save_mAP</span> <span class="o">=</span> <span class="s1">&#39;data/mAP/skimageBRIEF_notredame.npy&#39;</span><span class="p">,</span>
                             <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span>
                             <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;hamming&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;skimage BRIEF PS = </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s1">, mAP on Notredame = </span><span class="si">{</span><span class="n">mAP_BRIEF</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Found saved results data/mAP/skimageBRIEF_notredame.npy, loading
skimage BRIEF PS = 65, mAP on Notredame = 0.44817
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you use the benchmark, please cite it:</p>

<pre><code>@misc{BrownRevisited2020,
  title={UBC PhotoTour Revisied},
  author={Mishkin, Dmytro},
  year={2020},
  url = {https://github.com/ducha-aiki/brown_phototour_revisited}
}</code></pre>

</div>
</div>
</div>
</div>
 

