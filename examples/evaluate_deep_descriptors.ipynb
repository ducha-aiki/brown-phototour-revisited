{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep descriptors baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will evaluate all descriptors, which are available easily enough, e.g. from kornia or authors github implementation. There will be final comparison table in the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer 1: don't trust this table fully\n",
    "\n",
    "\n",
    "I haven't (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true -- and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please open an issue. Thank you. \n",
    "\n",
    "\n",
    "### Disclaimer 2: it is not \"benchmark\".\n",
    "\n",
    "\n",
    "The intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results  instead of doing so on [HPatches](https://github.com/hpatches/hpatches-benchmark). After you have finished tuning, please, evaluate your local descriptors on some downstream task like [IMC image matching benchmark](https://github.com/vcg-uvic/image-matching-benchmark) or [visual localization](https://www.visuallocalization.net/).\n",
    "\n",
    "\n",
    "**If you found any mistake, please open an issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RootSIFT reference\n",
    "\n",
    "Let's first add RootSIFT, which is not deep learned, but is the gold-standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import kornia\n",
    "from brown_phototour_revisited.benchmarking import *\n",
    "\n",
    "descs_out_dir = 'data/descriptors'\n",
    "download_dataset_to = 'data/dataset'\n",
    "results_dir = 'data/mAP'\n",
    "patch_size = 32\n",
    "\n",
    "full_results_dict = {}\n",
    "\n",
    "for patch_size in [32]:\n",
    "    desc_name = 'Kornia RootSIFT'\n",
    "    model = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval()\n",
    "    desc_dict = full_evaluation(model,\n",
    "                                desc_name,\n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                path_to_save_mAP = results_dir,\n",
    "                                patch_size = patch_size, \n",
    "                                device = torch.device('cuda:0'), \n",
    "                           distance='euclidean',\n",
    "                           backend='pytorch-cuda')\n",
    "    full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "\n",
    "clear_output()\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardNet and SOSNet, [kornia](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature) implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kornia provides only liberty-trained (best) weights, so we have to download the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/DagnyT/hardnet/raw/master/pretrained/train_yosemite_with_aug/checkpoint_yosemite_with_aug.pth\n",
    "!wget https://github.com/DagnyT/hardnet/raw/master/pretrained/train_notredame_with_aug/checkpoint_notredame_with_aug.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "desc_name = 'HardNet'\n",
    "hardnet_lib = kornia.feature.HardNet(True).eval()\n",
    "\n",
    "hardnet_notre = kornia.feature.HardNet(False)\n",
    "hardnet_notre.load_state_dict(torch.load('checkpoint_notredame_with_aug.pth')['state_dict'])\n",
    "                            \n",
    "hardnet_yos = kornia.feature.HardNet(False)\n",
    "hardnet_yos.load_state_dict(torch.load('checkpoint_yosemite_with_aug.pth')['state_dict'])\n",
    "\n",
    "models = {'liberty': hardnet_lib,\n",
    "         'notredame': hardnet_notre,\n",
    "         'yosemite': hardnet_yos}\n",
    "\n",
    "desc_dict = full_evaluation(models,\n",
    "                            desc_name,\n",
    "                            path_to_save_dataset = download_dataset_to,\n",
    "                            path_to_save_descriptors = descs_out_dir,\n",
    "                            path_to_save_mAP = results_dir,\n",
    "                            patch_size = patch_size, \n",
    "                            device = torch.device('cuda:0'), \n",
    "                       distance='euclidean',\n",
    "                       backend='pytorch-cuda')\n",
    "\n",
    "full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "clear_output()\n",
    "\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/yuruntian/SOSNet/raw/master/sosnet-weights/sosnet_32x32_notredame.pth\n",
    "!wget https://github.com/yuruntian/SOSNet/raw/master/sosnet-weights/sosnet_32x32_yosemite.pth\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
      "SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "desc_name = 'SOSNet'\n",
    "sosnet_lib = kornia.feature.SOSNet(True).eval()\n",
    "\n",
    "sosnet_notre = kornia.feature.SOSNet(False)\n",
    "sosnet_notre.load_state_dict(torch.load('sosnet_32x32_notredame.pth'))\n",
    "                            \n",
    "sosnet_yos = kornia.feature.SOSNet(False)\n",
    "sosnet_yos.load_state_dict(torch.load('sosnet_32x32_yosemite.pth'))\n",
    "\n",
    "models = {'liberty': sosnet_lib,\n",
    "         'notredame': sosnet_notre,\n",
    "         'yosemite': sosnet_yos}\n",
    "\n",
    "\n",
    "desc_dict = full_evaluation(models,\n",
    "                            desc_name,\n",
    "                            path_to_save_dataset = download_dataset_to,\n",
    "                            path_to_save_descriptors = descs_out_dir,\n",
    "                            path_to_save_mAP = results_dir,\n",
    "                            patch_size = patch_size, \n",
    "                            device = torch.device('cuda:0'), \n",
    "                       distance='euclidean',\n",
    "                       backend='pytorch-cuda')\n",
    "full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "clear_output()\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TFeat](https://github.com/vbalnt/tfeat)\n",
    "\n",
    "It is really light-weight and strong descriptor. We will copy-paste author implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/vbalnt/tfeat/blob/master/tfeat_model.py\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TNet(nn.Module):\n",
    "    \"\"\"TFeat model definition\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.InstanceNorm2d(1, affine=False),\n",
    "            nn.Conv2d(1, 32, kernel_size=7),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=6),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.descr = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.descr(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-liberty.params\n",
    "!wget https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-yosemite.params\n",
    "!wget https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-notredame.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
      "SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\n",
      "TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "desc_name = 'TFeat'\n",
    "tfeat_lib = TNet()\n",
    "tfeat_lib.load_state_dict(torch.load('tfeat-liberty.params'))\n",
    "\n",
    "tfeat_notre = TNet()\n",
    "tfeat_notre.load_state_dict(torch.load('tfeat-notredame.params'))\n",
    "                            \n",
    "tfeat_yos = TNet()\n",
    "tfeat_yos.load_state_dict(torch.load('tfeat-yosemite.params'))\n",
    "\n",
    "\n",
    "models = {'liberty': tfeat_lib,\n",
    "          'yosemite': tfeat_yos,\n",
    "          'notredame': tfeat_notre}\n",
    "desc_dict = full_evaluation(models,\n",
    "                            desc_name,\n",
    "                            path_to_save_dataset = download_dataset_to,\n",
    "                            path_to_save_descriptors = descs_out_dir,\n",
    "                            path_to_save_mAP = results_dir,\n",
    "                            patch_size = patch_size, \n",
    "                            device = torch.device('cuda:0'), \n",
    "                       distance='euclidean',\n",
    "                       backend='pytorch-cuda')\n",
    "full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "clear_output()\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Soft Margin HardNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/lg-zhang/dynamic-soft-margin-pytorch/raw/master/pretrained/liberty_float/model.state_dict -O dsm_lib.pth\n",
    "!wget https://github.com/lg-zhang/dynamic-soft-margin-pytorch/raw/master/pretrained/notredame_float/model.state_dict -O dsm_notre.pth\n",
    "!wget https://github.com/lg-zhang/dynamic-soft-margin-pytorch/raw/master/pretrained/yosemite_float/model.state_dict -O dsm_yos.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
      "SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\n",
      "TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24\n",
      "SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "desc_name = 'SoftMargin'\n",
    "tfeat_lib = kornia.feature.HardNet(False).eval()\n",
    "tfeat_lib.load_state_dict(torch.load('dsm_lib.pth'))\n",
    "\n",
    "tfeat_notre = kornia.feature.HardNet(False).eval()\n",
    "tfeat_notre.load_state_dict(torch.load('dsm_notre.pth'))\n",
    "                            \n",
    "tfeat_yos = kornia.feature.HardNet(False).eval()\n",
    "tfeat_yos.load_state_dict(torch.load('dsm_yos.pth'))\n",
    "\n",
    "\n",
    "models = {'liberty': tfeat_lib,\n",
    "          'yosemite': tfeat_yos,\n",
    "          'notredame': tfeat_notre}\n",
    "desc_dict = full_evaluation(models,\n",
    "                            desc_name,\n",
    "                            path_to_save_dataset = download_dataset_to,\n",
    "                            path_to_save_descriptors = descs_out_dir,\n",
    "                            path_to_save_mAP = results_dir,\n",
    "                            patch_size = patch_size, \n",
    "                            device = torch.device('cuda:0'), \n",
    "                       distance='euclidean',\n",
    "                       backend='pytorch-cuda')\n",
    "full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "clear_output()\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardNetPS\n",
    "\n",
    "HardNetPS is the HardNet version, trained on the [PS dataset](https://github.com/rmitra/PS-Dataset), which does very well on HPatches, but badly on IMC benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/DagnyT/hardnet/raw/master/pretrained/3rd_party/HardNetPS/HardNetPS.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
      "SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\n",
      "TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24\n",
      "SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63\n",
      "HardNetPS 32px         55.56              49.70               49.12 \n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class HardNetPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HardNetPS, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = True),\n",
    "        nn.BatchNorm2d(32, affine=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = True),\n",
    "        nn.BatchNorm2d(32, affine=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = True),\n",
    "        nn.BatchNorm2d(64, affine=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = True),\n",
    "        nn.BatchNorm2d(64, affine=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = True),\n",
    "        nn.BatchNorm2d(128, affine=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = True),\n",
    "        nn.BatchNorm2d(128, affine=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128, 128, kernel_size=8, bias = True)\n",
    "    )\n",
    "    def input_norm(self,x):\n",
    "        flat = x.view(x.size(0), -1)\n",
    "        mp = torch.mean(flat, dim=1)\n",
    "        sp = torch.std(flat, dim=1) + 1e-7\n",
    "        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_features = self.features(self.input_norm(input))\n",
    "        x = x_features.view(x_features.size(0), -1)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "\n",
    "desc_name = 'HardNetPS'\n",
    "model = HardNetPS().eval()\n",
    "model.load_state_dict(torch.load('HardNetPS.pth'))\n",
    "desc_dict = full_evaluation(model,\n",
    "                            desc_name,\n",
    "                            path_to_save_dataset = download_dataset_to,\n",
    "                            path_to_save_descriptors = descs_out_dir,\n",
    "                            path_to_save_mAP = results_dir,\n",
    "                            patch_size = patch_size, \n",
    "                            device = torch.device('cuda:0'), \n",
    "                       distance='euclidean',\n",
    "                       backend='pytorch-cuda')\n",
    "full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "clear_output()\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2D2\n",
    "\n",
    "That is NOT fair benchmark for R2D2 for 2 reasons. First, it is dense descriptor, which outputs 32x32 descriptor field for 32x32 patch. We take central pixel, which is kind of reasonable, but not the thing, R2D2 is trained for.\n",
    "Second, it expects RGB patches and Brown dataset is in grayscale. \n",
    "Anyway, let's try!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/naver/r2d2/master/nets/patchnet.py\n",
    "!wget https://github.com/naver/r2d2/raw/master/models/r2d2_WASF_N8_big.pt\n",
    "!wget https://github.com/naver/r2d2/raw/master/models/r2d2_WASF_N16.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quad_L2Net_ConfCFS(mchan=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Quad_L2Net_ConfCFS(\n",
       "  (ops): ModuleList(\n",
       "    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (10): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (13): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "    (16): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(192, 192, kernel_size=(2, 2), stride=(1, 1), padding=(2, 2), dilation=(4, 4))\n",
       "    (19): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (20): Conv2d(192, 192, kernel_size=(2, 2), stride=(1, 1), padding=(4, 4), dilation=(8, 8))\n",
       "    (21): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (22): Conv2d(192, 128, kernel_size=(2, 2), stride=(1, 1), padding=(8, 8), dilation=(16, 16))\n",
       "  )\n",
       "  (clf): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (sal): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patchnet import *\n",
    "\n",
    "R2D2 = Quad_L2Net_ConfCFS(mchan=6)\n",
    "weights = torch.load('r2d2_WASF_N8_big.pt')\n",
    "print (weights['net'])\n",
    "weights2 = {}\n",
    "for k, v in weights['state_dict'].items():\n",
    "    weights2[k.replace('module.','')] = v\n",
    "R2D2.load_state_dict(weights2)\n",
    "R2D2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2D2_Center(torch.nn.Module):\n",
    "    def __init__(self, r2d2):\n",
    "        super(R2D2_Center, self).__init__()\n",
    "        self.r2d2 = r2d2\n",
    "        return\n",
    "    def forward(self,x):\n",
    "        orig_out = self.r2d2([x.repeat(1,3,1,1)])\n",
    "        return orig_out['descriptors'][0][...,15,15]\n",
    "eval_r2d2 = R2D2_Center(R2D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
      "SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\n",
      "TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24\n",
      "SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63\n",
      "HardNetPS 32px         55.56              49.70               49.12 \n",
      "R2D2_center_grayscal   61.47              53.18               54.98 \n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "desc_name = 'R2D2_center_grayscale'\n",
    "desc_dict = full_evaluation(eval_r2d2,\n",
    "                            desc_name,\n",
    "                            path_to_save_dataset = download_dataset_to,\n",
    "                            path_to_save_descriptors = descs_out_dir,\n",
    "                            path_to_save_mAP = results_dir,\n",
    "                            patch_size = patch_size, \n",
    "                            device = torch.device('cuda:0'), \n",
    "                       distance='euclidean',\n",
    "                       backend='pytorch-cuda')\n",
    "full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "clear_output()\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get patch descriptors, is to take the mean over central descriptors. Let's try 2x2 window, as 32x32 doesn't have a single \"center\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
      "HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
      "SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\n",
      "TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24\n",
      "SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63\n",
      "HardNetPS 32px         55.56              49.70               49.12 \n",
      "R2D2_center_grayscal   61.47              53.18               54.98 \n",
      "R2D2_MeanCenter_gray   62.73              54.10               56.17 \n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class R2D2_MeanCenter(torch.nn.Module):\n",
    "    def __init__(self, r2d2):\n",
    "        super(R2D2_MeanCenter, self).__init__()\n",
    "        self.r2d2 = r2d2\n",
    "        return\n",
    "    def forward(self,x):\n",
    "        orig_out = self.r2d2([x.repeat(1,3,1,1)])\n",
    "        return F.normalize(orig_out['descriptors'][0][...,15:17, 15:17].mean(dim=-1).mean(dim=-1), p=2, dim=1)\n",
    "\n",
    "eval_r2d2 = R2D2_MeanCenter(R2D2)\n",
    "desc_name = 'R2D2_MeanCenter_gray'\n",
    "desc_dict = full_evaluation(eval_r2d2,\n",
    "                            desc_name,\n",
    "                            path_to_save_dataset = download_dataset_to,\n",
    "                            path_to_save_descriptors = descs_out_dir,\n",
    "                            path_to_save_mAP = results_dir,\n",
    "                            patch_size = patch_size, \n",
    "                            device = torch.device('cuda:0'), \n",
    "                       distance='euclidean',\n",
    "                       backend='pytorch-cuda')\n",
    "full_results_dict[f'{desc_name} {patch_size}px'] = desc_dict\n",
    "clear_output()\n",
    "print_results_table(full_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better, but not enough. OK, that was an unfair comparison anyway and R2D2 performed quite decently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the benchmark, please cite it:\n",
    "\n",
    "    @misc{BrownRevisited2020,\n",
    "      title={UBC PhotoTour Revisied},\n",
    "      author={Mishkin, Dmytro},\n",
    "      year={2020},\n",
    "      url = {https://github.com/ducha-aiki/brown_phototour_revisited}\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
