{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from brown_phototour_revisited.dataset import *\n",
    "from brown_phototour_revisited.extraction import *\n",
    "from brown_phototour_revisited.benchmarking import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  brown_phototour_revisited\n",
    "\n",
    "> The package for local patch descriptors evaluation, which takes into account image indexes and second nearest neighbor ratio (SNN) filtering strategy. It is in agreement with IMC benchmark and practice, unlike the original protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will become your README and also the index of your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install brown_phototour_revisited`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 main modules of the package: dataset, extraction and benchmarking. \n",
    "To run the benchmark one needs two things:\n",
    " - extract the desccriptors with either 'extract_pytorchinput_descriptors' or 'extract_numpyinput_descriptors'\n",
    " - get the mean average precision (mAP) with 'evaluate_mAP_snn_based'\n",
    " \n",
    "Here we will show how to evaluate several descriptors: Pytorch-based HardNet, OpenCV SIFT, skimage BRIEF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kornia in /home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/site-packages (0.4.1+95af063)\r\n",
      "Requirement already satisfied: numpy in /home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/site-packages (from kornia) (1.19.1)\r\n",
      "Requirement already satisfied: torch<1.7.0,>=1.6.0 in /home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/site-packages (from kornia) (1.6.0)\r\n",
      "Requirement already satisfied: future in /home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/site-packages (from torch<1.7.0,>=1.6.0->kornia) (0.18.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install kornia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will download the HardNet, trained on Liberty dataset, download the Notredame subset and extracts the local patch descriptors into the dict. Note, that we should not evaluate descriptor on the same subset, as it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/dataset/notredame.pt\n",
      "data/descriptors/HardNet+Liberty_32px_notredame.npy already exists, loading\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import kornia\n",
    "\n",
    "from brown_phototour_revisited.dataset import *\n",
    "from brown_phototour_revisited.extraction import *\n",
    "from brown_phototour_revisited.benchmarking import *\n",
    "\n",
    "model = kornia.feature.HardNet(True).eval()\n",
    "\n",
    "descs_out_dir = 'data/descriptors'\n",
    "download_dataset_to = 'data/dataset'\n",
    "patch_size = 32 # HardNet expects 32x32 patches\n",
    "\n",
    "desc_dict = extract_pytorchinput_descriptors(model,\n",
    "                                'HardNet+Liberty',\n",
    "                                subset= 'notredame', \n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                patch_size = patch_size, \n",
    "                                device = torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['descriptors', 'labels', 'img_idxs'])\n"
     ]
    }
   ],
   "source": [
    "print (desc_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **extract_pytorchinput_descriptors** expects **torch.nn.Module**, which takes (B, 1, patch_size, patch_size) torch.Tensor input and outputs (B, desc_dim) torch.Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved results data/mAP/HardNet+Liberty_notredame.npy, loading\n",
      "HardNetLib mAP on Notredame = 0.61938\n"
     ]
    }
   ],
   "source": [
    "mAP = evaluate_mAP_snn_based(desc_dict['descriptors'],\n",
    "                             desc_dict['labels'], \n",
    "                             desc_dict['img_idxs'],\n",
    "                             path_to_save_mAP = 'data/mAP/HardNet+Liberty_notredame.npy',\n",
    "                            backend='pytorch-cuda')\n",
    "print (f'HardNetLib mAP on Notredame = {mAP:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate OpenCV SIFT descriptor. \n",
    "Function **extract_numpyinput_descriptors** expects function or object, which takes (patch_size, patch_size) input and outputs (desc_dim) np.array.\n",
    "As OpenCV doesn't provide such function, we will create it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_center_kp(PS=65.):\n",
    "    c = PS/2.0\n",
    "    center_kp = cv2.KeyPoint()\n",
    "    center_kp.pt = (c,c)\n",
    "    center_kp.size = 2*c/5.303\n",
    "    return center_kp\n",
    "\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "center_kp = get_center_kp(32)\n",
    "\n",
    "def extract_opencv_sift(patch):\n",
    "    return sift.compute((255*patch).astype(np.uint8),[center_kp])[1][0].reshape(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/dataset/notredame.pt\n",
      "data/descriptors/OpenCV_SIFT_32px_notredame.npy already exists, loading\n"
     ]
    }
   ],
   "source": [
    "descs_out_dir = 'data/descriptors'\n",
    "download_dataset_to = 'data/dataset'\n",
    "patch_size = 32\n",
    "\n",
    "desc_dict_sift = extract_numpyinput_descriptors(extract_opencv_sift,\n",
    "                                'OpenCV_SIFT',\n",
    "                                subset= 'notredame', \n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                patch_size = patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved results data/mAP/OpenCV_SIFT_notredame.npy, loading\n",
      "OpenCV SIFT PS = 32 mAP on Notredame = 0.45925\n"
     ]
    }
   ],
   "source": [
    "mAP_SIFT = evaluate_mAP_snn_based(desc_dict_sift['descriptors'],\n",
    "                             desc_dict_sift['labels'], \n",
    "                             desc_dict_sift['img_idxs'],\n",
    "                            path_to_save_mAP = 'data/mAP/OpenCV_SIFT_notredame.npy',\n",
    "                            backend='pytorch-cuda')\n",
    "print (f'OpenCV SIFT PS = {patch_size} mAP on Notredame = {mAP_SIFT:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try some binary descriptor, like BRIEF. Evaluation so far supports two metrics: **euclidean** and **hamming**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/dataset/notredame.pt\n",
      "data/descriptors/skimage_BRIEF_32px_notredame.npy already exists, loading\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.feature import BRIEF\n",
    "\n",
    "BR = BRIEF(patch_size = patch_size)\n",
    "def extract_skimage_BRIEF(patch):\n",
    "    BR.extract(patch.astype(np.float64), np.array([patch_size/2.0, patch_size/2.0]).reshape(1,2))\n",
    "    return BR.descriptors.astype(np.float32)\n",
    "\n",
    "desc_dict_brief = extract_numpyinput_descriptors(extract_skimage_BRIEF,\n",
    "                                'skimage_BRIEF',\n",
    "                                subset= 'notredame', \n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                patch_size = patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's will take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='24' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      24.00% [24/100 02:31<07:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:681: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "mAP_BRIEF = evaluate_mAP_snn_based(desc_dict_brief['descriptors'],\n",
    "                             desc_dict_brief['labels'], \n",
    "                             desc_dict_brief['img_idxs'],\n",
    "                             path_to_save_mAP = 'data/mAP/skimage_BRIEF_notredame.npy',\n",
    "                             backend='pytorch-cuda',\n",
    "                             distance='hamming')\n",
    "print (f'skimage BRIEF PS = {patch_size} mAP on Notredame = {mAP_BRIEF:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. \n",
    "For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary. \n",
    "\n",
    "We have function, which does all these evaluations: **full_evaluation**, which internally calls the functions we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Caching data data/dataset/liberty.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='362' class='' max='1759' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.58% [362/1759 00:19<01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea261c48c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/old-ufo/anaconda3/envs/fastai1/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4eade94da10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                 \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                            \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                            backend='pytorch-cuda')\n\u001b[0m",
      "\u001b[0;32m~/dev/brown-phototour-revisited/brown_phototour_revisited/benchmarking.py\u001b[0m in \u001b[0;36mfull_evaluation\u001b[0;34m(models, desc_name, path_to_save_dataset, path_to_save_descriptors, patch_size, device, backend, distance)\u001b[0m\n\u001b[1;32m    138\u001b[0m                              distance=distance)\n\u001b[1;32m    139\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearned_on\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'{desc_name} trained on {trained_on} PS = {patch_size} mAP on {subset} = {mAP:.5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/brown-phototour-revisited/brown_phototour_revisited/extraction.py\u001b[0m in \u001b[0;36mextract_pytorchinput_descriptors\u001b[0;34m(model, desc_name, subset, path_to_save_dataset, path_to_save_descriptors, patch_size, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdescriptors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai1/lib/python3.7/site-packages/kornia/feature/siftdesc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mmag\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mori\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matan2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mmag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmag\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mo_big\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_ang_bins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mori\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import kornia\n",
    "from brown_phototour_revisited.benchmarking import *\n",
    "patch_size = 65 # SIFT performs better with bigger patch size.\n",
    "\n",
    "model = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval()\n",
    "\n",
    "descs_out_dir = 'data/descriptors'\n",
    "download_dataset_to = 'data/dataset'\n",
    "results_dir = 'data/mAP'\n",
    "desc_dict = full_evaluation(model,\n",
    "                                'Kornia RootSIFT',\n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                path_to_save_mAP = results_dir,\n",
    "                                patch_size = patch_size, \n",
    "                                device = torch.device('cuda:0'), \n",
    "                           distance='euclidean',\n",
    "                           backend='pytorch-cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
