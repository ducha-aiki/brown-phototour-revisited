{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from brown_phototour_revisited.dataset import *\n",
    "from brown_phototour_revisited.extraction import *\n",
    "from brown_phototour_revisited.benchmarking import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  brown_phototour_revisited\n",
    "\n",
    "> The package for local patch descriptors evaluation, which takes into account image indexes and second nearest neighbor ratio (SNN) filtering strategy. It is in agreement with IMC benchmark and practice, unlike the original protocol. The benchmark is not \"test benchmark\" by amy means. Rather it is intended to be used as validation/development set for local patch descriptor learning and/or crafting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install brown_phototour_revisited`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a single function, which does everything for you: `full_evaluation`. The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary.  We are following it here as well.\n",
    "\n",
    "However, if you need to run some tests separately, or reuse some functions -- we will cover the usage below.\n",
    "In the following example we will show how to use `full_evaluation` to evaluate SIFT descriptor as implemented in kornia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kornia in /home/mishkdmy/.local/lib/python3.7/site-packages (0.4.1+0c9e625)\n",
      "Requirement already satisfied: numpy in /home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages (from kornia) (1.18.0)\n",
      "Requirement already satisfied: torch<1.7.0,>=1.6.0 in /home/mishkdmy/.local/lib/python3.7/site-packages (from kornia) (1.6.0)\n",
      "Requirement already satisfied: future in /home/mishkdmy/.local/lib/python3.7/site-packages (from torch<1.7.0,>=1.6.0->kornia) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
      "------------------------------------------------------------------------------\n",
      "trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
      "tested  on           yosemite           notredame            liberty\n",
      "------------------------------------------------------------------------------\n",
      "Kornia RootSIFT        56.70              47.71               48.09 \n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import kornia\n",
    "from IPython.display import clear_output\n",
    "from brown_phototour_revisited.benchmarking import *\n",
    "patch_size = 65 \n",
    "\n",
    "model = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval()\n",
    "\n",
    "descs_out_dir = 'data/descriptors'\n",
    "download_dataset_to = 'data/dataset'\n",
    "results_dir = 'data/mAP'\n",
    "\n",
    "results_dict = {}\n",
    "results_dict['Kornia RootSIFT'] = full_evaluation(model,\n",
    "                                'Kornia RootSIFT',\n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                path_to_save_mAP = results_dir,\n",
    "                                patch_size = patch_size, \n",
    "                                device = torch.device('cuda:0'), \n",
    "                           distance='euclidean',\n",
    "                           backend='pytorch-cuda')\n",
    "clear_output()\n",
    "print_results_table(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precomputed benchmark results\n",
    "\n",
    "We have pre-computed some results for you. The implementation is in the following notebooks in the [examples](examples/) dir:\n",
    "\n",
    "- [Deep descriptors](examples/evaluate_deep_descriptors.ipynb)\n",
    "- [Non-deep descriptors](examples/evaluate_non_deep_descriptors.ipynb)\n",
    "\n",
    "The final table is the following:\n",
    "\n",
    "\n",
    "**If you found any mistake, please open an issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed examples of usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 main modules of the package: `dataset`, `extraction` and `benchmarking`. \n",
    "    \n",
    "To run the benchmark manually one needs two things:\n",
    " - extract the descriptors with either `extract_pytorchinput_descriptors` or `extract_numpyinput_descriptors`\n",
    " - get the mean average precision (mAP) with `evaluate_mAP_snn_based`\n",
    " \n",
    "Here we will show how to evaluate several descriptors: Pytorch-based HardNet, OpenCV SIFT, skimage BRIEF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will download the HardNet, trained on Liberty dataset, download the Notredame subset and extracts the local patch descriptors into the dict. Note, that we should not evaluate descriptor on the same subset, as it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/dataset/notredame.pt\n",
      "data/descriptors/HardNet+Liberty_32px_notredame.npy already exists, loading\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import kornia\n",
    "\n",
    "from brown_phototour_revisited.dataset import *\n",
    "from brown_phototour_revisited.extraction import *\n",
    "from brown_phototour_revisited.benchmarking import *\n",
    "\n",
    "model = kornia.feature.HardNet(True).eval()\n",
    "\n",
    "descs_out_dir = 'data/descriptors'\n",
    "download_dataset_to = 'data/dataset'\n",
    "patch_size = 32 # HardNet expects 32x32 patches\n",
    "\n",
    "desc_dict = extract_pytorchinput_descriptors(model,\n",
    "                                'HardNet+Liberty',\n",
    "                                subset= 'notredame', \n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                patch_size = patch_size, \n",
    "                                device = torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['descriptors', 'labels', 'img_idxs'])\n"
     ]
    }
   ],
   "source": [
    "print (desc_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `extract_pytorchinput_descriptors` expects `torch.nn.Module`, which takes `(B, 1, patch_size, patch_size)` `torch.Tensor` input and outputs `(B, desc_dim)` `torch.Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved results data/mAP/HardNet+Liberty_notredame.npy, loading\n",
      "HardNetLib mAP on Notredame = 0.61901\n"
     ]
    }
   ],
   "source": [
    "mAP = evaluate_mAP_snn_based(desc_dict['descriptors'],\n",
    "                             desc_dict['labels'], \n",
    "                             desc_dict['img_idxs'],\n",
    "                             path_to_save_mAP = 'data/mAP/HardNet+Liberty_notredame.npy',\n",
    "                            backend='pytorch-cuda')\n",
    "print (f'HardNetLib mAP on Notredame = {mAP:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate OpenCV SIFT descriptor. \n",
    "Function `extract_numpyinput_descriptors` expects function or object, which takes (patch_size, patch_size) input and outputs (desc_dim) np.array.\n",
    "\n",
    "As OpenCV doesn't provide such function, we will create it ourselves, or rather take already implemented from [HPatches benchmark repo](https://github.com/hpatches/hpatches-benchmark/blob/master/python/extract_opencv_sift.py#L43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/dataset/notredame.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='468159' class='' max='468159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [468159/468159 04:11<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "patch_size = 65\n",
    "\n",
    "# https://github.com/hpatches/hpatches-benchmark/blob/master/python/extract_opencv_sift.py#L43\n",
    "def get_center_kp(PS=65.):\n",
    "    c = PS/2.0\n",
    "    center_kp = cv2.KeyPoint()\n",
    "    center_kp.pt = (c,c)\n",
    "    center_kp.size = PS/5.303\n",
    "    return center_kp\n",
    "\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "center_kp = get_center_kp(patch_size)\n",
    "\n",
    "def extract_opencv_sift(patch):\n",
    "    #Convert back to UINT8 and provide aux keypoint in the center of the patch\n",
    "    return sift.compute((255*patch).astype(np.uint8),[center_kp])[1][0].reshape(128)\n",
    "\n",
    "descs_out_dir = 'data/descriptors'\n",
    "download_dataset_to = 'data/dataset'\n",
    "\n",
    "\n",
    "desc_dict_sift = extract_numpyinput_descriptors(extract_opencv_sift,\n",
    "                                'OpenCV_SIFT',\n",
    "                                subset= 'notredame', \n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                patch_size = patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved results data/mAP/OpenCV_SIFT65_notredame.npy, loading\n",
      "OpenCV SIFT PS = 65, mAP on Notredame = 0.45530\n"
     ]
    }
   ],
   "source": [
    "mAP_SIFT = evaluate_mAP_snn_based(desc_dict_sift['descriptors'],\n",
    "                             desc_dict_sift['labels'], \n",
    "                             desc_dict_sift['img_idxs'],\n",
    "                            path_to_save_mAP = 'data/mAP/OpenCV_SIFT65_notredame.npy',\n",
    "                            backend='pytorch-cuda')\n",
    "print (f'OpenCV SIFT PS = {patch_size}, mAP on Notredame = {mAP_SIFT:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try some binary descriptor, like BRIEF. Evaluation so far supports two metrics: `\"euclidean\"` and\n",
    "`\"hamming\"`.\n",
    "\n",
    "Function `extract_numpyinput_descriptors` expects function or object, which takes `(patch_size, patch_size)` input and outputs `(desc_dim)` `np.array`.\n",
    "\n",
    "As skimage doesn't provide exactly such function, we will create it ourselves by placing \"detected\" keypoint in the center of the patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/dataset/notredame.pt\n",
      "data/descriptors/skimage_BRIEF_65px_notredame.npy already exists, loading\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.feature import BRIEF\n",
    "patch_size = 65\n",
    "BR = BRIEF(patch_size = patch_size)\n",
    "def extract_skimage_BRIEF(patch):\n",
    "    BR.extract(patch.astype(np.float64), np.array([patch_size/2.0, patch_size/2.0]).reshape(1,2))\n",
    "    return BR.descriptors.astype(np.float32)\n",
    "\n",
    "desc_dict_brief = extract_numpyinput_descriptors(extract_skimage_BRIEF,\n",
    "                                'skimage_BRIEF',\n",
    "                                subset= 'notredame', \n",
    "                                path_to_save_dataset = download_dataset_to,\n",
    "                                path_to_save_descriptors = descs_out_dir,\n",
    "                                patch_size = patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's will take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved results data/mAP/skimageBRIEF_notredame.npy, loading\n",
      "skimage BRIEF PS = 65, mAP on Notredame = 0.44817\n"
     ]
    }
   ],
   "source": [
    "mAP_BRIEF = evaluate_mAP_snn_based(desc_dict_brief['descriptors'].astype(np.bool),\n",
    "                             desc_dict_brief['labels'], \n",
    "                             desc_dict_brief['img_idxs'],\n",
    "                             path_to_save_mAP = 'data/mAP/skimageBRIEF_notredame.npy',\n",
    "                             backend='numpy',\n",
    "                             distance='hamming')\n",
    "print (f'skimage BRIEF PS = {patch_size}, mAP on Notredame = {mAP_BRIEF:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the benchmark, please cite it:\n",
    "\n",
    "    @misc{BrownRevisited2020,\n",
    "      title={UBC PhotoTour Revisied},\n",
    "      author={Mishkin, Dmytro},\n",
    "      year={2020},\n",
    "      url = {https://github.com/ducha-aiki/brown_phototour_revisited}\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
